<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Robotic Arm with Computer Vision</title>
  <link rel="stylesheet" href="style.css">
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      line-height: 1.6;
      background-color: #121212;
      color: white;
    }

    header {
      position: relative;
      padding: 20px;
      background-color: #333;
      color: rgb(240, 170, 170);
      text-align: center;
    }

    .back-button {
      position: absolute;
      top: 20px;
      left: 20px;
      padding: 10px 15px;
      background: rgb(190, 230, 195);
      color: black;
      border: none;
      border-radius: 5px;
      cursor: pointer;
      font-size: 1rem;
    }

    #project-title {
      font-size: 3rem;
      margin: 0;
    }

    main {
      padding: 5px;
      max-width: 1200px;
      margin: 0 auto;
    }

    h1 {
      font-size: 3rem;
      margin-bottom: 20px;
    }

    h2 {
      font-size: 2rem;
      margin-bottom: 15px;
      color: white;
    }

    h3 {
      font-size: 1.5rem;
      margin-bottom: 10px;
      color: #ccc;
    }

    p {
      text-indent: 20px;
      margin-bottom: 20px;
      text-align: justify;
    }

    .image-gallery {
      display: flex;
      justify-content: center;
      gap: 20px;
      flex-wrap: wrap;
      margin-bottom: 20px;
    }

    .image-gallery img {
      max-width: 100%;
      height: auto;
      border-radius: 8px;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }

    .github-button {
      position: fixed;
      top: 50px;
      right: 20px;
      background: #333;
      border: none;
      padding: 10px;
      border-radius: 50%;
      cursor: pointer;
      display: flex;
      align-items: center;
      justify-content: center;
      width: 70px;
      height: 70px;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);
      z-index: 100;
    }

    .github-button img {
      width: 70px;
      height: 70px;
      margin: 0;
      border-radius: 0;
      box-shadow: none;
    }

    .menu-icon {
      position: fixed;
      top: 150px;
      right: 30px;
      background: #333;
      color: white;
      padding: 10px;
      border-radius: 5px;
      cursor: pointer;
      font-size: 1.2rem;
    }

    .popup-menu {
      display: none;
      position: fixed;
      top: 200px;
      right: 60px;
      background: #222;
      color: white;
      border: 1px solid #444;
      padding: 10px;
      border-radius: 5px;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);
    }

    .popup-menu a {
      color: #00bcd4;
      text-decoration: none;
      display: block;
      margin: 5px 0;
    }

    .popup-menu a:hover {
      text-decoration: underline;
    }

    footer {
      text-align: center;
      padding: 20px;
      background: #333;
      color: white;
    }
    .code-block {
  background-color: #241f1f;
  padding: 15px;
  margin: 20px 0;
  border-radius: 5px;
  overflow-x: auto;
  color: white;
  font-family: 'Courier New', Courier, monospace;
  line-height: 1.5;
  
}
code {
  display: block;
  white-space: pre-wrap;
  word-wrap: break-word;
}
.code-block {
  background-color: #1e1e1e; /* Darker background for contrast */
  color: #d4d4d4; /* Light gray text for better readability */
  font-family: 'Courier New', Courier, monospace; /* Classic monospace font */
  padding: 20px; /* Spacious padding for a clean look */
  border-radius: 8px; /* Rounded corners for modern design */
  margin: 20px 0; /* Consistent spacing */
  border: 1px solid #333; /* Subtle border for definition */
  overflow-x: auto; /* Horizontal scrolling for long lines of code */
  box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2); /* Slight shadow for depth */
}

pre {
  margin: 0; /* Remove default margins inside the code block */
  line-height: 1.5; /* Better line spacing for readability */
}

code {
  display: block; /* Ensure code wraps properly */
  white-space: pre-wrap; /* Allow code to wrap for better UX */
  word-break: break-word; /* Handle long, unbroken strings */
}

.subtitle {
  text-align: center; /* Center the text */
  color: #a0dde6; /* A distinct color (cyan in this case) */
  font-size: 1.8rem; /* Slightly smaller than main section titles */
  margin: 20px 0; /* Add spacing above and below */
  font-weight: bold; /* Make it stand out */
}
.subtitle_2 { /*for subtitle's subtitle*/
  text-align: center; /* Center the text */
  color: #a0dde6; /* A distinct color (cyan in this case) */
  font-size: 1.5rem; /* Slightly smaller than main section titles */
  margin: 20px 0; /* Add spacing above and below */
}

.image-container {
    display: flex; /* Arrange images side by side */
    justify-content: center; /* Center the images in the container */
    gap: 20px; /* Add space between the images */
    margin: 30px 0; /* Add space above and below the image container */
}

.image-container .image {
    max-width: 45%; /* Ensure images take up 45% of the container width each */
    height: auto; /* Maintain aspect ratio */
    border-radius: 8px; /* Slightly round the corners */
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2); /* Add a shadow for depth */
}

.caption-box {
    background-color: #333; /* Dark gray background */
    color: #ddd; /* Light gray text */
    font-size: 1.2rem; /* Slightly larger font for readability */
    text-align: center; /* Center-align the caption */
    padding: 15px; /* Add padding inside the box */
    margin: 20px auto; /* Center the box and add vertical spacing */
    max-width: 80%; /* Restrict width to make it look proportional */
    border-radius: 8px; /* Add rounded corners */
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3); /* Subtle shadow for pop-out effect */
    border: 1px solid #444; /* Add a border to separate it from the background */
}

/* I am using this to hyperlink the files*/
.highlight-link {
    text-decoration: none; /* Remove the default underline */
}

.highlight-link:hover {
    text-decoration: none; /* Ensure no underline appears on hover */
}

.highlight {
    background-color: #333; /* Dark gray background */
    color: #ddd; /* Light gray text */
    padding: 3px 6px; /* Padding around the word */
    border-radius: 4px; /* Smooth rounded edges */
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2); /* Subtle shadow for depth */
    font-weight: bold; /* Optional: make it stand out more */
    cursor: pointer; /* Change cursor to pointer to indicate it's clickable */
}

/* Math equation container styling */
.math-block {
    background-color: #333; /* Dark grey background */
    color: #ddd; /* Light grey text */
    font-family: 'Courier New', Courier, monospace;
    font-size: 1.2rem;
    text-align: center;
    padding: 15px;
    margin: 20px auto; /* Centered with margin */
    border-radius: 8px; /* Rounded corners */
    max-width: 80%; /* Restrict width */
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2); /* Slight shadow */
    border: 1px solid #444; /* Subtle border */
}

/* Example walkthrough styling */
.example-walkthrough {
    text-align: left; /* Align content to left */
    margin: 30px auto;
    padding: 20px;
    background-color: #1e1e1e; /* Dark background */
    color: #d4d4d4; /* Light grey text */
    border-radius: 8px;
    border: 1px solid #444;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);
    max-width: 90%;
    font-size: 1.1rem;
    line-height: 1.6;
}

.example-walkthrough h2 {
    text-align: center;
    color: #a0dde6; /* Cyan-like title */
    font-size: 1.8rem;
    margin-bottom: 20px;
}

.example-walkthrough ul {
    list-style-type: disc; /* Bullet points */
    margin-left: 20px;
}

.example-walkthrough .math-inline {
    background-color: #333;
    color: #ddd;
    font-family: 'Courier New', Courier, monospace;
    padding: 3px 6px;
    border-radius: 4px;
    margin: 0 3px;
}

.code-block {
    background-color: #1e1e1e;
    color: #d4d4d4;
    font-family: 'Courier New', Courier, monospace;
    padding: 20px;
    border-radius: 8px;
    margin: 20px auto;
    max-width: 90%;
    overflow-x: auto;
    border: 1px solid #444;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);
}

.code-block code {
    display: block;
    white-space: pre-wrap;
}

.video-container {
  display: flex; /* Enables flexbox */
  justify-content: center; /* Centers horizontally */
  margin: 20px 0; /* Adds space above and below */
}

.small-video {
  height: 500px; /* Keeps the current height */
  width: auto; /* Maintains aspect ratio */
  border-radius: 8px;
  box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
}


  </style>
</head>
<body>
  <header>
    <button onclick="goBack()" class="back-button">&larr; Back</button>
    <h1 id="project-title">Robotic Arm with Computer Vision</h1>
  </header>

  <main>
    <button class="github-button" onclick="window.open('https://github.com/nischalkharel/RoboticArm', '_blank')">
      <img src="git_logo.png" alt="GitHub">
    </button>

    <div class="menu-icon" onclick="toggleMenu()">≡</div>
    <div class="popup-menu" id="popup-menu">
      <a href="https://github.com/nischalkharel/RoboticArm/blob/main/arm.py" target="_blank">arm.py</a>
      <a href="https://github.com/nischalkharel/RoboticArm/blob/main/camera.py" target="_blank">camera.py</a>
      <a href="https://github.com/nischalkharel/RoboticArm/blob/main/calculate_angles_for.py" target="_blank">calculate_angles_for.py</a>
      <a href="https://github.com/nischalkharel/RoboticArm/blob/main/move_motor.py" target="_blank">move_motor.py</a>
      <a href="https://github.com/nischalkharel/RoboticArm/blob/main/audio_in.py" target="_blank">audio_in.py</a>
      <a href="https://github.com/nischalkharel/RoboticArm/blob/main/audio_out.py" target="_blank">audio_out.py</a>
      <a href="https://github.com/nischalkharel/RoboticArm/blob/main/KeyboardControlledArm.py" target="_blank">KeyboardControlledArm.py</a>
    </div>

    <section id="project-description">
      <h1>Design Overview</h1>
      <h2 class="subtitle">High-Level Description</h2>
      <p>For this project, I built a robotic arm that uses computer vision to detect, pick up, and place a block with a metal gripper. The block is identified using an ArUco marker, and the arm calculates its movements based on the marker’s position in 3D space. To make it more fun and interactive, I added voice control where I can say "activate now," and the arm not only starts moving but also responds with voice prompts through a speaker I added. This was probably my favorite part because it made the arm feel less like a machine and more like something you could interact with naturally.</p>

      <section id="original-design-concepts">
        <h2 class="subtitle">Original Design Concepts</h2>
        <p>When I started, my project idea was to create a robotic arm with a camera module to detect an object’s location and pick it up. At first, I planned to 3D print the arm parts and attach motors myself to build the entire structure from scratch. However, after discussing with Dr. Dickerson (my professor), I decided to use a pre-made arm shell and focus more on integrating motors, the camera module, and the software. This approach allowed me to prioritize the technical challenges of control and computer vision without being delayed by manufacturing issues.</p>
      
        <p>For the camera, I originally wanted to use a stereo camera for depth sensing since they are known for accurately measuring distances. I thought this would be crucial for ensuring precise movements of the arm. However, after experimenting, I found that the stereo camera setup was challenging to configure and calibrate. While I did manage to get it to detect and track objects using OpenCV’s checkerboard calibration technique, it wasn’t significantly more accurate than a simpler setup using a single Arducam with ArUco markers. So, I ultimately decided to use the Arducam, which was more reliable for my application. If you’re curious, I’ve uploaded my stereo camera experiments to my GitHub repository, where you can check out how I set it up.</p>
      
        <p>Initially, I also considered including precise object placement as part of the arm’s functionality. However, I realized this would make the project scope too broad for the given timeline, so I decided to focus solely on object detection and pickup. Instead of placing the object at a specific location, I kept the task simpler by having the arm place it in a pre-defined area.</p>
      
        <h3 class="subtitle_2">Changes Made from the Proposal</h3>
        <ul>
          <li><strong>Camera Placement:</strong> I hadn’t initially decided where to place the camera, but during testing, I mounted it on the base of the arm in a custom enclosure. This position gave a stable and unobstructed view of the workspace.</li>
          <li><strong>Pre-Made Arm:</strong> I shifted from the idea of 3D printing the arm to using a pre-made shell, which allowed me to focus on coding and integrating components like the servos and camera module.</li>
          <li><strong>Simplified Task:</strong> I scaled back the complexity by focusing only on picking up objects rather than precise placement.</li>
        </ul>
      </section>
      
      <h2 class="subtitle">Final Design</h2>
      <p>The final design brings together hardware and software in a system that works smoothly:</p>
      
      <ul>
        <li><strong>Arm and Gripper:</strong> The arm is made of pre-built pieces, and the metal gripper is servo-controlled to pick up and release the block.</li>
        <li><strong>Camera:</strong> An Arducam mounted on the arm base detects the block using the ArUco library. I adjusted the library’s outputs for better distance accuracy. The camera was calibrated using OpenCV’s checkerboard calibration method.</li>
        <li><strong>Motors:</strong> All arm joints are powered by servo motors, which are controlled using the Adafruit ServoKit library for precise movements.</li>
        <li><strong>Voice Interaction:</strong> I added a microphone and speaker for voice control and feedback. Saying "activate now" triggers the arm, which responds with phrases like “Arm activated” before starting its routine.</li>
        <li><strong>Control Software:</strong> The project is driven by a set of Python files that work together like this:
          <ul>
            <li><a href="https://github.com/nischalkharel/RoboticArm/blob/main/arm.py" class="highlight-link">
              <span class="highlight"><strong>arm.py:</strong></span></a> This is the main file that ties everything together. It listens for voice commands and coordinates the overall process, from detecting the block to moving the arm.</li>
            <li><a href="https://github.com/nischalkharel/RoboticArm/blob/main/camera.py" class="highlight-link">
              <span class="highlight"><strong>camera.py:</strong></span></a> This handles detecting the ArUco marker, calculating the block's position in x, y, z, and returning those coordinates.</li>
            <li><a href="https://github.com/nischalkharel/RoboticArm/blob/main/calculate_angles_for.py" class="highlight-link">
              <span class="highlight"><strong>calculate_angles_for.py:</strong></span></a> Using the block’s coordinates, this file calculates the angles for each joint of the arm. I simplified inverse kinematics into something more intuitive for me, which worked better than traditional IK.</li>
            <li><a href="https://github.com/nischalkharel/RoboticArm/blob/main/move_motor.py" class="highlight-link">
              <span class="highlight"><strong>move_motor.py:</strong></span></a> This file sends the calculated angles to the servos, making the arm move smoothly.</li>
            <li><a href="https://github.com/nischalkharel/RoboticArm/blob/main/audio_in.py" class="highlight-link">
              <span class="highlight"><strong>audio_in.py</strong></span></a> and <a href="https://github.com/nischalkharel/RoboticArm/blob/main/audio_out.py" class="highlight-link">
                <span class="highlight"><strong>audio_out.py:</strong></span></a> These files handle the voice interaction. One listens for commands through the microphone, and the other sends audio feedback through the speaker.</li>
          </ul>
        </li>
      </ul>

      <div class="image-gallery">
        <img src="arm_transparent.png" alt="Robotic Arm Design" style="width: 300px; height: auto;">
        <div class="caption-box">
            <p>The image above is of the final design. I have the speaker and camera right in the front plate of my small enclosure. The mic is on the right side of the arm sticking out. Also, you can see how the (pink) servos were used. Also in the back you can see the block with ArUco that I used, but I will have more pictures of it later in the blog.</p>
        </div>
      </div>
      <h2 class="subtitle">Expanding on Previous Work</h2>
      <p>This project builds on a mix of existing tools and my own modifications:</p>
      <ul>
        <li><strong>Servo Control:</strong> I initially tried manually programming servo movements, but they were too jerky. The Adafruit ServoKit library made a huge difference, letting me smoothly move the joints to exact angles.</li>
        <li><strong>Custom IK Logic:</strong> I spent days trying to understand traditional inverse kinematics. While I got it partially working, it wasn’t accurate enough for my setup. So, I developed a simpler version of IK that skips matrices and uses trigonometry that made more sense to me. (I’ll explain the math later in this blog.)</li>
        <li><strong>ArUco Markers:</strong> Using the ArUco library, I detected and calculated distances to the block. However, the library’s output wasn’t perfect, so I tweaked the distance values manually to improve accuracy.</li>
        <li><strong>Stereo Camera:</strong> Although I didn’t use the stereo camera in the final setup, it was a great learning experience. I calibrated it using OpenCV’s checkerboard calibration and got it working, but it wasn’t more accurate than the Arducam setup. You can find my stereo camera code and experiments on my GitHub repository if you’re curious.</li>
      </ul>

      <section id="ik-glimpse">
        <div class="image-container">
            <img src="ipad_math.jpg" alt="Handwritten IK Math Notes" class="image">
            <img src="ipad_math_arm.jpg" alt="IK Applied to Arm Diagram" class="image">
        </div>
        <div class="caption-box">
            <p>These are some glimpses of how I expanded on what I learned about inverse kinematics (IK) to make it simpler for my robotic arm. I will explain these more in later sections.</p>
        </div>
    </section>
    

    
      <h2 class="subtitle">Purpose</h2>
      <p>This project was a way for me to dive deeper into robotics, math, and computer vision. It’s not just a robotic arm—it’s a system where hardware and software work together in sync. The voice control and feedback added personality to the arm, making it more interactive and fun to use. I’m really proud of what I built, and there’s so much potential to expand it further, like sorting objects or even playing chess.</p>
    </section>
  



    <section id="preliminary-design-verification">
      <h1>Preliminary Design Verification</h1>
  
      <h2 class="subtitle">Assembly and Servo Integration</h2>
      <p>The robotic arm was assembled using pre-built components and servos. These servos were connected to a Raspberry Pi through a old servo controller that I had (I will try to find the exact type on amazon and put the link at the end of the blog) and controlled using the Adafruit ServoKit library. Each servo was responsible for specific movements: the base for horizontal rotation, the shoulder and elbow for vertical positioning, the wrist for up-and-down movement, and the gripper for opening and closing. This setup provided precise control over the arm’s movements.</p>
      
      <div class="video-container">
        <video class="small-video" controls>
          <source src="prototype.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    

      <h2 class="subtitle">Motor Testing and Manual Calibration</h2>
      <p>Before moving to advanced features like computer vision and inverse kinematics, I wrote a program (<a href="https://github.com/nischalkharel/RoboticArm/blob/main/KeyboardControlledArm.py" class="highlight-link">
    <span class="highlight"><strong>KeyboardControlledArm.py</strong></span></a>) to manually control the arm's motors. The program mapped keyboard keys to specific joints for precise movement testing. For example:</p>
      <ul>
          <li><strong>w/s:</strong> Control the shoulder's up-and-down motion.</li>
          <li><strong>a/d:</strong> Control the wrist's up-and-down movement.</li>
          <li><strong>Arrow Keys:</strong> Control the base rotation (left/right) and the elbow's up-and-down motion.</li>
          <li><strong>o/c:</strong> Open and close the gripper.</li>
      </ul>
      <p>The program also displayed real-time angles of all servos on the command line, helping me understand how angle adjustments corresponded to the arm's physical movements.</p>
  
      <p>Here is an example of how the program mapped keyboard inputs to control the servos:</p>
      <div class="code-block">
          <pre><code>
  <a href="https://github.com/nischalkharel/RoboticArm/blob/main/KeyboardControlledArm.py" class="highlight-link">
    <span class="highlight"><strong>KeyboardControlledArm.py</strong></span></a>
  # Shoulder control
  if key == ord('w'):
      if shoulder_angle < SHOULDER_MAX:
          shoulder_angle += ANGLE_STEP
  elif key == ord('s'):
      shoulder_angle -= ANGLE_STEP
  
  # Wrist control
  if key == ord('a'):
      if wrist_angle > WRIST_MIN:
          wrist_angle -= ANGLE_STEP
  elif key == ord('d'):
      if wrist_angle < WRIST_MAX:
          wrist_angle += ANGLE_STEP
          </code></pre>
      </div>
  
      <h2 class="subtitle">Key Features of the Testing Program</h2>
      <p>The testing program had several key features:</p>
      <ul>
          <li>Real-time angle display on the command line.</li>
          <li>Dynamic angle limit adjustments to prevent collisions.</li>
          <li>Smooth movements achieved through small angle increments.</li>
          <li>Intuitive keyboard controls for easy testing and calibration.</li>
      </ul>
  
      <h2 class="subtitle">Testing Outcomes</h2>
      <p>This program helped me:</p>
      <ul>
          <li>Understand how changes in angles correspond to physical movements.</li>
          <li>Identify safe operating ranges for each servo.</li>
          <li>Ensure smooth, realistic motion by fine-tuning angle adjustments.</li>
      </ul>
      <div class="image-gallery">
        <img src="keyboardResults.png" alt="Robotic Arm Design" style="width: 500px; height: auto;">
        <div class="caption-box">
          <p>The above image shows the printouts from Keyboard controlled program, and these indicate the current angles of each motor. It was really helpful to be able to see this change as I moved each motors.</p>
        </div>
        </div>
      
        <p>Below is an example of how the program updated the servo angles in real time:</p>
      <div class="code-block">
          <pre><code>
  <a href="https://github.com/nischalkharel/RoboticArm/blob/main/KeyboardControlledArm.py" class="highlight-link">
    <span class="highlight"><strong>KeyboardControlledArm.py</strong></span></a>
  # Update the servos with the new angles
  kit.servo[0].angle = base_angle
  kit.servo[1].angle = shoulder_angle
  kit.servo[2].angle = elbow_angle
  kit.servo[3].angle = wrist_angle
  kit.servo[4].angle = gripper_angle
          </code></pre>
      </div>
  
      <h2 class="subtitle">Getting Things to Work in Real Life</h2>
      <p>This testing phase gave me a better understanding of how the arm moves and reacts. Manually controlling the motors helped me identify calibration issues, like how small errors in servo movements could affect the gripper’s ability to pick up objects. These early tests built a strong foundation for moving to computer vision and automated joint angle calculations.</p>
  </section>
  



  <section id="design-implementation">
    <h1>Design Implementation</h1>

    <h2 class="subtitle">Overall System Overview</h2>
    <p>The robotic arm system integrates three main areas: computer vision for block detection, motor control for precise arm movements, and voice interaction for user commands. The camera, mounted on the arm's base, captures the workspace and detects ArUco markers to determine the block's position. These coordinates (x, y, z) are used to calculate joint angles, which the servos then execute to pick up the block and place it in a predefined location. The voice command feature allows the system to feel more interactive, enabling activation with a simple phrase, “activate now,” followed by verbal feedback from the arm.</p>

    <h2 class="subtitle">Subcomponents Developed/Used</h2>
    <ul>
        <li><strong>Hardware:</strong>
            <ul>
                <li><strong>Robotic Arm Shell:</strong> A pre-built arm structure, modified with servo motors for the joints and gripper.</li>
                <li><strong>Servo Motors:</strong> Five motors responsible for the base rotation, shoulder, elbow, wrist, and gripper movement.</li>
                <li><strong>Camera:</strong> An Arducam mounted on the arm's base, configured to detect ArUco markers for block localization.</li>
                <li><strong>Audio Input and Output:</strong>
                    <ul>
                        <li><strong>Microphone:</strong> Captures user commands for interaction.</li>
                        <li><strong>Speaker:</strong> Provides voice feedback to users.</li>
                    </ul>
                </li>
            </ul>
        </li>
        <div class="image-gallery">
          <img src="arm_pic.png" alt="Robotic Arm Design" style="width: 300px; height: auto;">
        </div>
        <li><strong>Software:</strong>
            <ul>
                <li><strong>Camera System:</strong> OpenCV’s ArUco library for marker detection and distance measurement. The <a href="https://github.com/nischalkharel/RoboticArm/blob/main/camera.py" class="highlight-link">
                  <span class="highlight"><strong>camera.py</strong></span></a> module handles real-time image processing and calculates block coordinates.</li>
                <li><strong>Motor Control:</strong> The <a href="https://github.com/nischalkharel/RoboticArm/blob/main/calculate_angles_for.py" class="highlight-link">
                  <span class="highlight"><strong>calculate_angles_for.py</strong></span></a> module computes joint angles using a simplified inverse kinematics approach. The <a href="https://github.com/nischalkharel/RoboticArm/blob/main/move_motor.py" class="highlight-link">
                    <span class="highlight"><strong>move_motor.py</strong></span></a> module handles smooth servo movements, ensuring precise arm positioning.</li>
                <li><strong>Voice Interaction:</strong> Speech recognition via speech_recognition library to detect user commands, and espeak for generating responses from the arm.</li>
                <li><strong>Main Controller:</strong> The <a href="https://github.com/nischalkharel/RoboticArm/blob/main/arm.py" class="highlight-link">
                  <span class="highlight"><strong>arm.py</strong></span></a> file integrates all subsystems, acting as the central logic hub.</li>
            </ul>
        </li>
    </ul>
    

    <h2 class="subtitle">Design Process and Challenges</h2>
    <p>This project was my first real experience using a Raspberry Pi on my own (I had used it in a class before, but it was a structured lab with step-by-step instructions, so I didn’t get much room to experiment). Diving into this project was a whole new level of trial and error, especially because I was trying to figure out everything from scratch.</p>
    <ol>
        <li><strong>Camera System Development:</strong>
            <p>At first, I thought using a stereo camera would be the easiest way to measure depth and detect the block. Boy, was I wrong! The stereo camera I ordered didn’t have much documentation, so I spent hours—no, days—trying to get it to work. I installed so many libraries, tried countless configurations, and just when I thought it would never work, I finally managed to get it running. But even then, I wasn’t out of the woods.</p>
            <div class="image-gallery">
              <img src="stereo.png" alt="Robotic Arm Design" style="width: 600px; height: auto;">
              <div class="caption-box">
                <p>My original idea was to use the camera to recognize the block's color (blue in my case) and estimate its distance by comparing its size in the image to its actual size. That worked... kind of. Until the lighting changed. Then everything fell apart. It was so inconsistent that I couldn’t trust it at all.</p>
              </div>
            </div>
            <p>After some research, I stumbled upon ArUco markers, and let me tell you, they were a game changer. They made everything way more consistent by providing clear features for the camera to detect, even with changing lighting. Using OpenCV’s ArUco library, I was able to calculate the block’s position based on the marker’s size and orientation:</p>
            <div class="code-block">
                <pre><code>
# Detect ArUco markers and estimate pose
corners, ids, _ = aruco.detectMarkers(gray, aruco_dict, parameters=parameters)
if ids is not None:
    rvecs, tvecs, _ = aruco.estimatePoseSingleMarkers(corners, 0.0235, camera_matrix, dist_coeffs)
    for i in range(len(ids)):
        x_m, y_m, z_m = tvecs[i][0]  # Extract (x, y, z) in meters
        x_in, y_in, z_in = x_m * 39.3701, y_m * 39.3701, z_m * 39.3701  # Convert to inches
        return x_in, y_in, z_in
                </code></pre>
            </div>
        </li>
        <li><strong>Switching to a Single Camera:</strong>
            <p>Once I got the markers working with the stereo camera, I had the thought, “Do I even need the stereo camera?” Turns out, I didn’t. A single camera with ArUco markers worked just as well for my setup. It simplified everything—no more messing with two camera feeds, less processing power needed, and fewer headaches overall.</p>
        </li>

        <div class="image-gallery">
          <img src="aruco.png" alt="Robotic Arm Design" style="width: 600px; height: auto;">
          <div class="caption-box">
              <p>This image shows ArUco marker being detected by the Arducam.</p>
          </div>
        </div>
        <li><strong>Voice Interaction:</strong>
          <p>One of the coolest parts of the project was adding voice commands. I used <code>speech_recognition</code> to listen for commands like “activate now” and <code>espeak</code> to make the arm respond with phrases like “Arm activating!” This feature made the system feel more alive and interactive, which I really enjoyed.</p>
      </li>
        <li><strong>Motor Control and Inverse Kinematics:</strong>
            <p>Once I had the block’s position figured out, the next challenge was moving the arm to pick it up. Instead of using traditional matrix-based inverse kinematics (IK), which felt way too complicated for me, I came up with a simpler method using trigonometry. This approach let me calculate the joint angles without diving into the deep end of IK theory.</p>
            <div class="code-block">
                <pre><code>
# Smoothly move a motor to the specified angle
def move_motor(motor_id, target_angle):
    step = 1 if target_angle > current_angle else -1
    for angle in range(current_angle, target_angle, step):
        kit.servo[motor_id].angle = angle
        sleep(0.02)
    kit.servo[motor_id].angle = target_angle
                </code></pre>
            </div>
        </li>
        <section id="simplified-ik">
          <h1>From Theory to Reality: Solving IK Step by Step</h1>
          <p>
            Moving the robotic arm accurately to pick up a block placed on the ground wasn’t as straightforward as I first thought. Most of the inverse kinematics (IK) explanations I found online were too theoretical, and applying them directly to my arm setup didn’t work. After many hours of testing, trial, and error, I developed a simplified IK approach specific to my system.
          </p>
          <p>
            Here’s a step-by-step breakdown of how I calculate the angles for each motor, adjust the wrist motor’s position using <span class="highlight">G<sub>z</sub></span> and <span class="highlight">G<sub>y</sub></span>, and ensure the shoulder and elbow angles stay within their safe ranges.
          </p>
          <p>This is mostly going through <a href="https://github.com/nischalkharel/RoboticArm/blob/main/calculate_angles_for.py" class="highlight-link">
            <span class="highlight"><strong>calculate_angles_for.py:</strong></span></a></p>
          <!-- Step-by-Step Explanation -->
          <h2 class="subtitle">Understanding G<sub>y</sub> and G<sub>z</sub></h2>
          <p>
            Initially, I assumed the wrist motor would be **vertically aligned above the block**. In this position:
          </p>
          <div class="caption-box">
            G<sub>y</sub> = 6.5 inches (vertical distance)<br>
            G<sub>z</sub> = 0 inches (horizontal offset from the block).
          </div>
          <p>
            If the distance from the shoulder to the wrist motor (target point) cannot be reached with the given arm lengths <span class="highlight">L<sub>2</sub></span> (shoulder to elbow) and <span class="highlight">L<sub>3</sub></span> (elbow to wrist), I gradually adjust <span class="highlight">G<sub>z</sub></span> by increasing it in small increments (1% of Z). As a result, <span class="highlight">G<sub>y</sub></span> decreases because the direct distance from the wrist to the block must always stay constant at 6.5 inches.
          </p>
          <p>The following code handles this adjustment:</p>
          <div class="code-block">
            <pre><code>
        G_z = 0  # Start with no horizontal offset
        G_y = GRIPPER_LENGTH  # Vertical distance initially equals gripper length
        
        while G_z <= z:
            shoulder_to_target_y = L1 - G_y  # Height from shoulder to wrist
            shoulder_to_target_distance = math.sqrt(x**2 + z**2) - G_z  # Horizontal distance
        
            # Calculate the total distance (R) from shoulder to wrist
            R = math.sqrt(shoulder_to_target_y**2 + shoulder_to_target_distance**2)
        
            # Check if the target is reachable
            if R <= (L2 + L3):
                # Proceed with angle calculations
                break
            else:
                G_z += 0.01 * z  # Increment G_z
                G_y = math.sqrt(GRIPPER_LENGTH**2 - G_z**2)  # Adjust G_y
            </code></pre>
          </div>
        
          <!-- Step-by-Step Math Explanation -->
          <h2 class="subtitle">Example Walkthrough</h2>
          <p>
            Let’s assume we want the robotic arm to grab a block located at:
          </p>
          <div class="caption-box">(x, y, z) = (3, 0.5, 7)</div>
          <p>Here’s how the calculations proceed:</p>
        
          <ul>
            <li>
              <strong>Step 1:</strong> Calculate the **base motor angle** using the horizontal offset.
              <div class="caption-box">
                θ<sub>base</sub> = atan2(3, 7) = 23.2°<br>
                Adjust for motor offset: θ<sub>base</sub> = 90 - 23.2 = 66.8°
              </div>
            </li>

            <div class="image-gallery">
              <img src="base_angle.jpg" alt="Robotic Arm Design" style="width: 500px; height: auto;">
              <div class="caption-box">
                <p>Here you can visually see what X = 3 inches and Z = 7 inches looks like. Using arctan I calculate the angle the base would have to rotate inorder to face the block directly. Also I show the hypotneuse calculation because it is used in later steps.</p>
              </div>
            </div>

            <li>
              <strong>Step 2:</strong> Initialize <span class="highlight">G<sub>y</sub></span> and <span class="highlight">G<sub>z</sub></span>. Here I am assuming the gripper is lined up straight up form the block like in the image below.
              <div class="caption-box">
                G<sub>z</sub> = 0 inches<br>
                G<sub>y</sub> = 6.5 inches
              </div>
            </li>

            <section id="ik-glimpse">
              <div class="image-container">
                  <img src="straight.jpg" alt="Handwritten IK Math Notes" class="image">
                  <img src="angled.jpg" alt="IK Applied to Arm Diagram" class="image">
              </div>
              <div class="caption-box">
                  <p>Left image shows the initial position where G<sub>z</sub> = 0 inches and G<sub>y</sub> = 6.5 inches.</p>
                  <p>Right image shows a situtaion where we haven't found the working angles, so we keep increasing G<sub>z</sub> and G<sub>y</sub>.</p>
              </div>
          </section>

            <li>
              <strong>Step 3:</strong> Calculate the **height** and **horizontal distance** from the shoulder to the wrist motor.
              <div class="caption-box">
                H = L<sub>1</sub> + G<sub>y</sub> - Y = 4 + 6.5 - 4 = 3.5 inches<br>
                D = √(X² + Z²) - G<sub>z</sub> = √(3² + 7²) - 0 = 7.62 inches
              </div>
            </li>

            <li><strong>Step 4:</strong> Check if <span class="math-inline">R</span> is reachable. If <span class="math-inline">R > (L<sub>2</sub> + L<sub>3</sub>)</span>, increment <span class="math-inline">G<sub>z</sub></span> to tilt the wrist and reduce <span class="math-inline">G<sub>y</sub></span>.</li>

              <div class="caption-box">
                R = √(H² + D²) = √(3.5² + 7.62²) = 8.4 inches <br>
                **if this distance is higher than L_2 and L_3 combined then I just keep increasing G_z and trying to get this distance lower.**
              </div>
            </li>
            <div class="image-gallery">
              <img src="shoulder_elbow.jpg" alt="Robotic Arm Design" style="width: 900px; height: auto;">
              <div class="caption-box">
                <p>In the image above, you can see how we can use the 7.62 inches found above (and in step 1 image) to calculate the <strong>distance</strong> from the shoulder to the wrist motor.</p>
              </div>
            </div>
            <li>
              <strong>Step 5:</strong> Use the **law of cosines** to calculate the angles for the elbow and shoulder motors.
              <div class="caption-box">
                cos(θ<sub>elbow</sub>) = (L<sub>2</sub>² + L<sub>3</sub>² - R²) / (2 * L<sub>2</sub> * L<sub>3</sub>)<br>
                cos(θ<sub>shoulder</sub>) = (L<sub>2</sub>² + R² - L<sub>3</sub>²) / (2 * L<sub>2</sub> * R)
              </div>
            </li>
            <div class="image-gallery">
              <img src="angles.jpg" alt="Robotic Arm Design" style="width: 900px; height: auto;">
              <div class="caption-box">
                <p>We can then now use L_2 (measured and stays constant), L_3 (measured and stays constant), and shoulder to wrist distance (calculated above) to find angle in elbow and shoulder.</p>
                <p>I then compare these angles to the min and max that I recordered from the preliminary design verification section to verify if these angles are in the range.</p>
                <div class="image-gallery">
                  <img src="measured_data.png" alt="Robotic Arm Design" style="width: 600px; height: auto;">
                </div>
              </div>
            </div>

          </ul>
        
          <p>The final angles for the motors are compared to their pre-defined safe ranges:</p>
          <div class="code-block">
            <pre><code>
        if SHOULDER_MIN <= theta_shoulder_deg <= SHOULDER_MAX and ELBOW_MIN <= theta_elbow_deg <= ELBOW_MAX:
            realShoulderAngle = theta_shoulder_deg - 85
            realElbowAngle = 245 - theta_elbow_deg
            print("Valid angles found.")
            return theta_base_deg, realShoulderAngle, realElbowAngle, theta_wrist_deg
        else:
            print("Angles out of range. Adjusting G_z further.")
            </code></pre>
          </div>
        
          <!-- Summary -->
          <h2 class="subtitle">Why This Works</h2>
          <p>
            By incrementally adjusting <span class="highlight">G<sub>z</sub></span> and recalculating <span class="highlight">G<sub>y</sub></span>, the system ensures that the robotic arm can reach as far as possible without exceeding its mechanical limits. If no valid angles are found after adjusting, the program concludes that the block is unreachable.
          </p>
          <p>
            This simplified IK method allowed me to achieve smooth and consistent movements without diving deep into advanced control theory. It uses trigonometry and iteration to solve a complex problem step by step.
          </p>
          <p>Also this example happened to work in just one try, but if the block was further away, we would have to loop around and constantly increase G_z until we can get the R (distance from shoulder to wrist) to be lower than L_2 + L_3.</p>

        </section>
        
    </ol>
</section>


<section id="design-testing">
  <h1>Design Testing</h1>
  <p>
      Once I had everything working — the camera, motor control, and IK calculations — I wanted to test if the robotic arm could consistently pick up a block placed in random spots. My goal was to ensure the arm could detect the block, calculate the angles, and move smoothly to grab it without any glitches.
  </p>

  <h2 class="subtitle_2">Testing Process</h2>
  <p>
      For testing, I placed the block with an ArUco marker in front of the arm at different random distances and angles. I would then say <strong>“activate now”</strong> to trigger the arm. The process looked like this:
  </p>
  <ul>
      <li><strong>Step 1:</strong> Place the block somewhere in front of the arm.</li>
      <li><strong>Step 2:</strong> Say “activate now” to start the system.</li>
      <li><strong>Step 3:</strong> Watch as the camera detects the marker, calculates the angles, and moves the arm to grab the block.</li>
  </ul>

  <p>
      I ran this test over and over with random placements to make sure the arm worked consistently. The only thing I had to be careful about was keeping the ArUco marker <strong>visible to the camera</strong>. If the marker was hidden or turned away, the camera couldn’t detect the block, and the arm wouldn’t move.
  </p>

  <p>
      To handle this, I wrote a loop in my code that only allows the arm to move if the camera detects the marker. If I say <strong>“activate now”</strong> and there’s no block (or the marker isn’t visible), the arm just does nothing. I don’t have a video for that case because, well, nothing happens!
  </p>

  <div style="text-align: center; margin: 20px 0;">
    <iframe width="800" height="450" src="https://www.youtube.com/embed/_Ltb7v08fbA" 
            title="Robotic arm picking up block with computer vision" frameborder="0" allowfullscreen>
    </iframe>
    <div class="caption-box">
      <p>This video is from initial tests before I added the ability to do voice control and the speaker, otherwise, everything else is the same.</p>
    </div>
</div>

  <h2 class="subtitle_2">Testing Results</h2>
  <p>
      I’m really happy with how the arm performed. Once the marker was in view, the arm moved smoothly and consistently to pick up the block. I tested it in several scenarios:
  </p>
  <ul>
      <li><strong>Random Distances:</strong> I placed the block closer (around 5 inches) and further away (around 12 inches).</li>
      <li><strong>Different Angles:</strong> I placed the block off to the side and directly in front of the arm to test the base rotation.</li>
      <li><strong>No Block Scenarios:</strong> When no block was present, the arm didn’t move at all — exactly as expected.</li>
  </ul>

  <h2 class="subtitle_2">Challenges</h2>
  <p>
      The biggest challenge was keeping the ArUco marker visible. At certain angles, the marker would tilt slightly, and the camera couldn’t detect it. I solved this by ensuring the block was placed flat with the marker always facing the camera. 
  </p>
  <p>
      Other than that, the system worked smoothly. Switching to ArUco markers from stereo cameras and color detection paid off big time because it made everything much more reliable.
  </p>

  <h2 class="subtitle_2">Video Demonstration</h2>
  <p>
      Below is a video showing multiple trials of the arm detecting the block and successfully picking it up. This video is a <strong>one-take recording</strong> to demonstrate that the arm works <strong>consistently</strong> without any cuts or edits.
  </p>
  <div style="text-align: center; margin: 20px 0;">
      <iframe width="800" height="450" src="https://www.youtube.com/embed/APHYfmsYTsw" 
              title="Robotic Arm Testing" frameborder="0" allowfullscreen>
      </iframe>
  </div>

  <h2 class="subtitle_2">Summary</h2>
  <p>
      Testing showed that the robotic arm works exactly as I wanted:
  </p>
  <ul>
      <li>If the ArUco marker is visible, the arm moves smoothly to the block and picks it up.</li>
      <li>If no block is detected, the arm stays in place, avoiding unnecessary movements.</li>
  </ul>
  <p>
      This consistency was the main reason I switched to using ArUco markers instead of relying on stereo color detection. Overall, I’m really happy with how it turned out!
  </p>
</section>


<section id="conclusion-future">
  <h1 class="subtitle">Conclusion and Future Work</h1>
  <p>
      After finishing this project, I realized I learned much more than I initially set out to. Beyond just building a robotic arm with computer vision, this project taught me how to work on a solo project and figure things out on my own. There were so many moments when I felt completely lost, with no idea what to do next. But I had to push through, research, test, and sometimes just keep trying until something worked. Looking back, I think learning how to troubleshoot and problem-solve without being told exactly what to do has been the most valuable part of this project.
  </p>
  <p>
      That said, I did achieve my technical goals too. I got to play around with <strong>computer vision</strong> and really understand how I can use it for detecting and interacting with objects. I also got hands-on experience with the <strong>Raspberry Pi</strong>, which was a big highlight for me. I hadn’t worked with it much before, but now I feel more confident using it, and I’m excited to explore more of its capabilities in future projects.
  </p>

  <h2 class="subtitle_2">Future Plans</h2>
  <p>
      For the future, I already have an idea for my next project: I want to take everything I learned from this and build a new robotic arm that can <strong>play chess with me</strong>. I have a general plan for what I want to do — combining computer vision to detect the board and pieces, integrating AI to calculate moves, and using inverse kinematics (IK) to move the arm. It’s definitely going to be a challenge, but I’m hoping to take this project as a starting point and push myself even further. I haven’t had time to start on it yet, but I’m really looking forward to it as my next big project.
  </p>

  <h2 class="subtitle_2">Summary</h2>
  <p>
      Overall, I’m really proud of what I accomplished with this project. It works consistently, does what I set out for it to do, and I learned so much along the way. This project gave me a solid foundation in <strong>computer vision</strong>, <strong>robotics</strong>, and working with <strong>Raspberry Pi</strong>, and I can’t wait to see how I’ll use those skills in the future.
  </p>
</section>


  
</main>


  <script>
    function goBack() {
      window.history.back();
    }

    function toggleMenu() {
      const menu = document.getElementById('popup-menu');
      menu.style.display = menu.style.display === 'block' ? 'none' : 'block';
    }
  </script>
</body>
</html>

